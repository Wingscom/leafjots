# Plan 08-01: CEX Import Infrastructure — DB Models, Upload API, Import History

## Metadata
- **Phase**: 8 — CEX Import Infrastructure
- **Requirements**: CIMP-01, CIMP-02, CIMP-03, CIMP-04, CIMP-05, CIMP-06, IMUI-01
- **Depends on**: Phase 7 complete (entity scoping)
- **Estimated files**: 10 modified, 6 new

## Goal
Create the database infrastructure to store CSV imports with full audit trail (raw rows preserved), a new upload API endpoint that stores every CSV row, and an Import History page in the UI. After this plan, uploading a CSV stores all raw data and shows import history — ready for parsers in Phase 9.

## Context

### What exists
- `CEXWallet` model: exchange, api_key_encrypted, api_secret_encrypted, last_trade_id
- `POST /api/wallets/{id}/import-csv` endpoint: reads CSV file, calls `BinanceCSVImporter`
- `BinanceCSVImporter`: parses OLD format (`Date(UTC)`, `Pair`, `Side`) — NOT the Transaction History format
- Frontend: file picker on Wallets page per CEX wallet, `importCSV()` API function, `useImportCSV()` hook
- Real CSV: `legacy_code/docs/binance_export.csv` — 340 rows, format: `User_ID, UTC_Time, Account, Operation, Coin, Change, Remark`
- Alembic HEAD: `v2_001`

### What needs to change
- New `CsvImport` model: stores import metadata (entity_id, exchange, filename, row_count, status)
- New `CsvImportRow` model: stores each raw CSV row linked to its import (for audit + re-parse)
- New Alembic migration for both tables
- New `/api/imports` router: upload endpoint (POST), list endpoint (GET)
- Rework upload flow: store raw rows first, then parse later (Phase 9)
- New Import page in UI with upload form + history table

### CSV Format (Binance Transaction History)
```
User_ID,UTC_Time,Account,Operation,Coin,Change,Remark
123456,2024-01-15 10:30:00,Spot,Transaction Buy,BTC,0.001,""
123456,2024-01-15 10:30:00,Spot,Transaction Spend,USDT,-35.50,""
123456,2024-01-15 10:30:00,Spot,Transaction Fee,BTC,-0.0000075,""
```

## Tasks

### Task 1: Create CsvImport and CsvImportRow models
**New file**: `src/cryptotax/db/models/csv_import.py`

```python
import uuid
from datetime import datetime
from typing import Optional

from sqlalchemy import ForeignKey, String, Text, Integer, BigInteger
from sqlalchemy.orm import Mapped, mapped_column, relationship

from cryptotax.db.session import Base, TimestampMixin, UUIDPrimaryKey


class CsvImport(UUIDPrimaryKey, TimestampMixin, Base):
    """Metadata for a CSV file upload."""
    __tablename__ = "csv_imports"

    entity_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("entities.id"))
    exchange: Mapped[str] = mapped_column(String(50), default="binance")
    filename: Mapped[str] = mapped_column(String(255))
    row_count: Mapped[int] = mapped_column(Integer, default=0)
    parsed_count: Mapped[int] = mapped_column(Integer, default=0)
    error_count: Mapped[int] = mapped_column(Integer, default=0)
    status: Mapped[str] = mapped_column(String(20), default="uploaded")
    # status: uploaded -> parsing -> completed -> error

    rows: Mapped[list["CsvImportRow"]] = relationship(
        back_populates="csv_import", cascade="all, delete-orphan", lazy="selectin"
    )


class CsvImportRow(UUIDPrimaryKey, Base):
    """A single raw CSV row stored for audit trail and re-parsing."""
    __tablename__ = "csv_import_rows"

    import_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("csv_imports.id"))
    row_number: Mapped[int] = mapped_column(Integer)
    # Raw CSV fields stored individually for easy querying
    utc_time: Mapped[str] = mapped_column(String(30))
    account: Mapped[str] = mapped_column(String(50))
    operation: Mapped[str] = mapped_column(String(100))
    coin: Mapped[str] = mapped_column(String(20))
    change: Mapped[str] = mapped_column(String(50))  # stored as string, parsed to Decimal by parser
    remark: Mapped[Optional[str]] = mapped_column(Text, default=None)
    # Parse result tracking
    status: Mapped[str] = mapped_column(String(20), default="pending")
    # status: pending -> parsed -> error -> skipped
    error_message: Mapped[Optional[str]] = mapped_column(Text, default=None)
    journal_entry_id: Mapped[Optional[uuid.UUID]] = mapped_column(
        ForeignKey("journal_entries.id"), default=None
    )

    csv_import: Mapped["CsvImport"] = relationship(back_populates="rows")
```

### Task 2: Register models in __init__.py
**File**: `src/cryptotax/db/models/__init__.py`

Add imports:
```python
from cryptotax.db.models.csv_import import CsvImport, CsvImportRow
from cryptotax.db.models.wallet import CEXWallet  # was missing
```

Add to `__all__`: `CsvImport`, `CsvImportRow`, `CEXWallet`

### Task 3: Create Alembic migration
**New file**: `alembic/versions/v3_001_csv_import_tables.py`

Migration `v3_001` (down_revision = `v2_001`):
- Creates `csv_imports` table with all columns from CsvImport model
- Creates `csv_import_rows` table with all columns from CsvImportRow model
- Adds indexes: `ix_csv_imports_entity_id`, `ix_csv_import_rows_import_id`, `ix_csv_import_rows_operation`

### Task 4: Create CsvImportRepo
**New file**: `src/cryptotax/db/repos/csv_import_repo.py`

```python
class CsvImportRepo:
    async def create_import(self, entity_id, exchange, filename, rows_data) -> CsvImport
    async def get_by_id(self, import_id) -> CsvImport | None
    async def list_for_entity(self, entity_id, limit=50, offset=0) -> tuple[list[CsvImport], int]
    async def update_status(self, import_id, status, parsed_count=None, error_count=None) -> None
    async def get_rows(self, import_id, status=None, limit=100, offset=0) -> list[CsvImportRow]
```

`create_import` method:
1. Creates `CsvImport` record
2. For each row in `rows_data`, creates `CsvImportRow` with raw fields parsed from dict
3. Sets `row_count` on the import
4. Returns the import with rows loaded

### Task 5: Create import API schemas
**New file**: `src/cryptotax/api/schemas/imports.py`

```python
class CsvImportResponse(BaseModel):
    id: uuid.UUID
    entity_id: uuid.UUID
    exchange: str
    filename: str
    row_count: int
    parsed_count: int
    error_count: int
    status: str
    created_at: datetime

class CsvImportListResponse(BaseModel):
    imports: list[CsvImportResponse]
    total: int

class CsvImportRowResponse(BaseModel):
    id: uuid.UUID
    row_number: int
    utc_time: str
    account: str
    operation: str
    coin: str
    change: str
    remark: str | None
    status: str
    error_message: str | None

class CsvImportDetailResponse(CsvImportResponse):
    rows: list[CsvImportRowResponse]  # only for detail view

class UploadResponse(BaseModel):
    import_id: uuid.UUID
    filename: str
    row_count: int
    status: str
```

### Task 6: Create imports API router
**New file**: `src/cryptotax/api/imports.py`

Router prefix: `/api/imports`

Endpoints:

| Method | Path | Action |
|--------|------|--------|
| POST | `/api/imports/upload` | Upload CSV file, store raw rows |
| GET | `/api/imports` | List imports for entity (with entity_id query param) |
| GET | `/api/imports/{import_id}` | Get import detail with row summary |
| GET | `/api/imports/{import_id}/rows` | Get rows for an import (with optional status filter) |

**POST `/upload`** implementation:
1. Accept `UploadFile` + `entity_id` (form field or query param) + `exchange` (default "binance")
2. Read file content as UTF-8 string
3. Parse with `csv.DictReader`
4. Validate required columns exist: `UTC_Time`, `Account`, `Operation`, `Coin`, `Change`
5. For each row, extract raw fields into a dict list
6. Call `csv_import_repo.create_import(entity_id, exchange, filename, rows_data)`
7. Commit and return `UploadResponse`

**GET `/`** implementation:
- Uses `resolve_entity` dependency for entity scoping
- Calls `csv_import_repo.list_for_entity(entity.id)`
- Returns `CsvImportListResponse`

### Task 7: Wire imports router into main.py
**File**: `src/cryptotax/api/main.py`

```python
from cryptotax.api.imports import router as imports_router
app.include_router(imports_router)
```

### Task 8: Create frontend imports API client
**New file**: `web/src/api/imports.ts`

```typescript
export interface CsvImport {
  id: string
  entity_id: string
  exchange: string
  filename: string
  row_count: number
  parsed_count: number
  error_count: number
  status: string
  created_at: string
}

export interface CsvImportList {
  imports: CsvImport[]
  total: number
}

export interface UploadResponse {
  import_id: string
  filename: string
  row_count: number
  status: string
}

export async function uploadCsv(file: File, entityId: string, exchange: string = 'binance'): Promise<UploadResponse> {
  const formData = new FormData()
  formData.append('file', file)
  formData.append('entity_id', entityId)
  formData.append('exchange', exchange)
  const res = await fetch('/api/imports/upload', { method: 'POST', body: formData })
  if (!res.ok) throw new Error(`Upload failed: ${res.status}`)
  return res.json()
}

export async function listImports(entityId?: string): Promise<CsvImportList> {
  return apiFetch(withEntityId('/imports', entityId))
}

export async function getImportDetail(importId: string): Promise<CsvImport> {
  return apiFetch(`/imports/${importId}`)
}
```

### Task 9: Create useImports hook
**New file**: `web/src/hooks/useImports.ts`

- `useImports()` — query, reads entityId from EntityContext, key: `['imports', entityId]`
- `useUploadCsv()` — mutation, invalidates `['imports']` on success
- `useImportDetail(importId)` — query for single import detail

### Task 10: Create Import page
**New file**: `web/src/pages/Imports.tsx`

Layout:
```
┌──────────────────────────────────────────────────┐
│ CSV Import                                       │
├──────────────────────────────────────────────────┤
│ Upload: [Choose CSV File] Exchange: [Binance ▾]  │
│         [Upload]                                 │
├──────────────────────────────────────────────────┤
│ Import History                                   │
│──────────────────────────────────────────────────│
│ Filename     │ Date       │ Rows │ Parsed │ Status│
│──────────────│────────────│──────│────────│───────│
│ export.csv   │ 2026-02-18 │ 340  │ 0      │ uploaded│
│ old_data.csv │ 2026-02-17 │ 120  │ 120    │ completed│
└──────────────────────────────────────────────────┘
```

Features:
- File upload form with drag-and-drop area
- Exchange selector (currently only Binance)
- Upload button triggers `useUploadCsv()` mutation
- Import history table showing all imports for the selected entity
- Status badges: uploaded (gray), parsing (yellow), completed (green), error (red)
- Row counts: total / parsed / errors

### Task 11: Add Imports route and nav item
**File**: `web/src/App.tsx` — add `/imports` route
**File**: `web/src/components/Layout.tsx` — add "Imports" nav item with `Upload` icon from lucide-react, positioned after "Entities" in the nav list

### Task 12: Write backend tests
**File**: `tests/unit/test_csv_import.py` (new)

Test:
1. `POST /api/imports/upload` with valid CSV — creates import with correct row_count
2. `GET /api/imports?entity_id=X` — returns only X's imports
3. `GET /api/imports/{id}` — returns import detail
4. Upload with invalid CSV (missing required columns) — returns 400
5. CsvImportRepo.create_import stores all raw row fields correctly
6. CsvImportRepo.list_for_entity with pagination works

## Verification
1. `python -m pytest tests/ -x -q` — all tests pass
2. `ruff check src/` — 0 lint errors
3. `cd web && npx tsc --noEmit` — 0 TypeScript errors
4. Manual: Upload `legacy_code/docs/binance_export.csv` → import created with 340 rows
5. Manual: `GET /api/imports` returns import history for entity
6. Manual: Import page shows upload form + history table

## Risks
- **Large CSV files**: Synchronous upload for now — sufficient for <10K rows. Phase 11 adds async progress.
- **CsvImportRow lazy loading**: relationship uses `selectin` on CsvImport.rows, but list endpoint should NOT load rows (only import metadata). Detail endpoint loads rows.
