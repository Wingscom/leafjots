# Plan 09-01: Binance CSV Transaction Parser + Parse Trigger API

## Metadata
- **Phase**: 9 — Binance CSV Parser -- Core Operations
- **Requirements**: BCSV-01, BCSV-02, BCSV-03, BCSV-04, BCSV-05, BCSV-06, BCSV-16, BCSV-17
- **Depends on**: Phase 8 (CsvImport/CsvImportRow models, upload API, csv_import_repo)
- **Estimated files**: 6 new, 4 modified

## Goal
Build a BinanceCsvParser that processes CsvImportRow records from Phase 8, groups multi-row transactions by timestamp, and produces balanced journal entries for the 6 core operation categories. Add a POST /api/imports/{id}/parse endpoint to trigger parsing.

## Context

### What exists (after Phase 8)
- `CsvImport` + `CsvImportRow` models with per-row status tracking (pending/parsed/error/skipped)
- `CsvImportRepo` with create_import, get_rows, update_status
- `POST /api/imports/upload` stores raw CSV rows
- `JournalEntry` + `JournalSplit` models with entity_id, balance validation
- `AccountMapper` with `cex_asset()`, `cex_expense()`, `income()`, `external_transfer()` methods
- `ParsedSplit` / `ParseResult` types from parser engine
- `CEXWallet` model (exchange field, polymorphic_identity="cex")
- Existing `BinanceTradeParser` handles API-fetched trades (qty/quoteQty/symbol format) — NOT the CSV Transaction History format

### Real CSV patterns (from legacy_code/docs/binance_export.csv)

**Spot Buy** (same UTC_Time, 3 rows):
```
Transaction Buy,  BTC,   +0.00008000
Transaction Spend, USDT, -6.92245840
Transaction Fee,  BTC,   -8E-8
```

**Spot Sell** (same UTC_Time, 3 rows):
```
Transaction Sold,    XRP,  -2.90000000
Transaction Revenue, USDT, +5.42358000
Transaction Fee,     USDT, -0.00542358
```

**Binance Convert** (same UTC_Time, 2 rows):
```
Binance Convert, BTC,  -0.00001000
Binance Convert, ETH,  +0.00029406
```
Note: Convert can span 2 accounts (Spot+Funding) — up to 4 rows at same timestamp.

**Deposit/Withdraw** (single row):
```
Deposit,  USDT, +29.90000000
Withdraw, ETH,  -0.00146957  (remark: "Withdraw fee is included")
```

**P2P Trading** (single row, Funding account):
```
P2P Trading, USDT, +7.18000000  (remark: P2P order ID)
```

**Internal Transfers** (same UTC_Time, 2 rows, mirrored accounts):
```
Spot:    Transfer Between Main and Funding Wallet, USDT, -6.93476480
Funding: Transfer Between Main and Funding Wallet, USDT, +6.93476480
```
Transfer types: Main↔Funding, Spot↔UM Futures, Spot↔CM Futures, Spot↔Options, Main↔Margin

### What needs to be built
1. `BinanceCsvParser` service — loads CsvImportRows, groups by UTC_Time, dispatches to operation handlers
2. Operation handlers for 6 core categories producing ParsedSplit lists
3. `CsvBookkeeper` — creates JournalEntry+Splits from ParsedSplits, links back to CsvImportRow
4. Parse trigger endpoint — POST /api/imports/{id}/parse
5. CEXWallet auto-creation (get-or-create per entity+exchange)
6. Tests with real CSV data patterns

## Tasks

### Task 1: Add CexAsset polymorphic identity to Account model
**File**: `src/cryptotax/db/models/account.py`

The `cex_asset` subtype is used in AccountMapper but has no polymorphic class. Add:
```python
class CexAsset(Account):
    __mapper_args__ = {"polymorphic_identity": "cex_asset"}
```

This prevents SQLAlchemy warnings and ensures STI works correctly for CEX accounts.

### Task 2: Create BinanceCsvParser service
**New file**: `src/cryptotax/parser/cex/binance_csv.py`

Core class that orchestrates CSV row parsing:

```python
class BinanceCsvParser:
    """Parse Binance Transaction History CSV rows into journal entries."""

    def __init__(self, session: AsyncSession, entity_id: uuid.UUID, wallet: Wallet) -> None:
        self._session = session
        self._entity_id = entity_id
        self._wallet = wallet
        self._mapper = AccountMapper(session)

    async def parse_import(self, csv_import: CsvImport) -> ParseStats:
        """Parse all pending rows in an import. Returns stats."""
        rows = await self._get_pending_rows(csv_import.id)
        groups = self._group_by_timestamp(rows)

        stats = ParseStats(total=len(rows))
        for utc_time, group_rows in groups.items():
            try:
                entries = self._parse_group(utc_time, group_rows)
                for entry_data in entries:
                    journal_entry = await self._create_journal_entry(entry_data, utc_time)
                    # Link rows to journal entry
                    for row in entry_data.source_rows:
                        row.status = "parsed"
                        row.journal_entry_id = journal_entry.id
                    stats.parsed += len(entry_data.source_rows)
            except Exception as e:
                for row in group_rows:
                    if row.status == "pending":
                        row.status = "error"
                        row.error_message = str(e)
                        stats.errors += 1

        return stats

    def _group_by_timestamp(self, rows: list[CsvImportRow]) -> dict[str, list[CsvImportRow]]:
        """Group rows by UTC_Time. Same timestamp = one logical transaction."""
        groups: dict[str, list[CsvImportRow]] = {}
        for row in rows:
            groups.setdefault(row.utc_time, []).append(row)
        return groups

    def _parse_group(self, utc_time: str, rows: list[CsvImportRow]) -> list[ParsedEntry]:
        """Dispatch a group of rows to the correct handler. May produce 1+ entries."""
        operations = {r.operation for r in rows}

        # Spot buy/sell (Transaction Buy/Spend/Fee or Sold/Revenue/Fee)
        if operations & {"Transaction Buy", "Transaction Spend"}:
            return [self._handle_spot_trade(rows)]
        if operations & {"Transaction Sold", "Transaction Revenue"}:
            return [self._handle_spot_trade(rows)]
        if all(r.operation == "Binance Convert" for r in rows):
            return [self._handle_convert(rows)]
        if any(r.operation == "Deposit" for r in rows):
            return [self._handle_deposit(r) for r in rows if r.operation == "Deposit"]
        if any(r.operation == "Withdraw" for r in rows):
            return [self._handle_withdraw(r) for r in rows if r.operation == "Withdraw"]
        if any(r.operation == "P2P Trading" for r in rows):
            return [self._handle_p2p(r) for r in rows if r.operation == "P2P Trading"]
        if any("Transfer Between" in r.operation for r in rows):
            return [self._handle_internal_transfer(rows)]

        # Unknown — mark all as error
        raise ValueError(f"Unknown operation group: {operations}")
```

Key design decisions:
- Groups rows by exact UTC_Time string match (not datetime parsing) — Binance guarantees same-second grouping
- Each handler returns a `ParsedEntry` dataclass with splits + entry_type + source_rows
- Mixed groups (e.g. buy + internal transfer at same timestamp) are handled by checking operation priority
- Unknown operations raise ValueError, caught by caller and recorded as error

### Task 3: Implement core operation handlers
**File**: `src/cryptotax/parser/cex/binance_csv.py` (continued)

**Spot Trade handler** (BCSV-01, BCSV-02):
```python
def _handle_spot_trade(self, rows: list[CsvImportRow]) -> ParsedEntry:
    """Transaction Buy/Spend/Fee OR Transaction Sold/Revenue/Fee."""
    splits: list[ParsedSplit] = []
    for row in rows:
        amount = Decimal(row.change)
        op = row.operation
        if op in ("Transaction Buy", "Transaction Sold"):
            splits.append(ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount))
        elif op in ("Transaction Spend", "Transaction Revenue"):
            splits.append(ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount))
        elif op == "Transaction Fee":
            # Fee is negative in CSV, split into asset decrease + expense increase
            splits.append(ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount))
            splits.append(ParsedSplit(account_subtype="wallet_expense", symbol=row.coin, quantity=-amount))
        else:
            splits.append(ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount))
    return ParsedEntry(splits=splits, entry_type="SWAP", source_rows=rows)
```

Balance check: Buy +0.00008 BTC, Spend -6.922 USDT, Fee -8E-8 BTC + expense +8E-8 BTC
- BTC: +0.00008 - 0.00000008 + 0.00000008 = +0.00008 ← NOT zero per-symbol
- This is correct behavior — SWAP entries don't balance per-symbol, they balance per-value (USD/VND)
- The existing `_validate_balance` in Bookkeeper checks per-symbol, but for CEX we need VALUE-based balance
- **Decision**: For CSV-sourced entries, skip per-symbol balance validation. The real balance validation is that all asset changes are recorded. Mark entry as balanced.

Actually, re-examining the CSV data more carefully:
- "Transaction Buy" BTC +0.00008 means the user RECEIVED 0.00008 BTC
- "Transaction Spend" USDT -6.922 means the user SPENT 6.922 USDT
- "Transaction Fee" BTC -0.00000008 means the user PAID a fee of 0.00000008 BTC

For double-entry accounting, each row is ONE split (one leg):
- cex_asset:BTC +0.00008 (received BTC)
- cex_asset:USDT -6.922 (spent USDT)
- cex_asset:BTC -0.00000008 (fee deducted from BTC balance)
- wallet_expense:BTC +0.00000008 (fee expense recorded)

These 4 splits are 2 symbols:
- BTC net = +0.00008 - 0.00000008 + 0.00000008 = +0.00008 (not zero)
- USDT net = -6.922 (not zero)

This is intentional for SWAP — the asset change IS the trade. We must NOT validate per-symbol zero-sum for swaps.

**Convert handler** (BCSV-03):
```python
def _handle_convert(self, rows: list[CsvImportRow]) -> ParsedEntry:
    """Binance Convert: 2+ rows, one positive (buy), one negative (sell)."""
    splits = []
    for row in rows:
        amount = Decimal(row.change)
        splits.append(ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount))
    return ParsedEntry(splits=splits, entry_type="SWAP", source_rows=rows)
```

**Deposit handler** (BCSV-04):
```python
def _handle_deposit(self, row: CsvImportRow) -> ParsedEntry:
    """Single row: coin deposited to exchange."""
    amount = Decimal(row.change)
    splits = [
        ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount),
        ParsedSplit(
            account_subtype="external_transfer", symbol=row.coin, quantity=-amount,
            account_params={"ext_address": "deposit"},
        ),
    ]
    return ParsedEntry(splits=splits, entry_type="DEPOSIT", source_rows=[row])
```

**Withdraw handler** (BCSV-04):
```python
def _handle_withdraw(self, row: CsvImportRow) -> ParsedEntry:
    """Single row: coin withdrawn (negative change, fee included)."""
    amount = Decimal(row.change)  # negative
    splits = [
        ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount),
        ParsedSplit(
            account_subtype="external_transfer", symbol=row.coin, quantity=-amount,
            account_params={"ext_address": "withdrawal"},
        ),
    ]
    return ParsedEntry(splits=splits, entry_type="WITHDRAWAL", source_rows=[row])
```

**P2P handler** (BCSV-05):
```python
def _handle_p2p(self, row: CsvImportRow) -> ParsedEntry:
    """P2P Trading: fiat-to-crypto buy on Funding account."""
    amount = Decimal(row.change)
    splits = [
        ParsedSplit(account_subtype="cex_asset", symbol=row.coin, quantity=amount),
        ParsedSplit(
            account_subtype="external_transfer", symbol=row.coin, quantity=-amount,
            account_params={"ext_address": "p2p"},
        ),
    ]
    return ParsedEntry(splits=splits, entry_type="DEPOSIT", source_rows=[row])
```

**Internal Transfer handler** (BCSV-06):
```python
def _handle_internal_transfer(self, rows: list[CsvImportRow]) -> ParsedEntry:
    """Transfer Between accounts: 2 mirrored rows (one +, one -)."""
    splits = []
    for row in rows:
        amount = Decimal(row.change)
        # Use account name in the subtype params to distinguish sub-accounts
        splits.append(ParsedSplit(
            account_subtype="cex_asset", symbol=row.coin, quantity=amount,
            account_params={"sub_account": row.account},
        ))
    return ParsedEntry(splits=splits, entry_type="TRANSFER", source_rows=rows)
```
Note: Internal transfers sum to zero per-symbol (mirrored rows). No tax event.

### Task 4: Create CsvBookkeeper service
**New file**: `src/cryptotax/accounting/csv_bookkeeper.py`

Lightweight bookkeeper for CSV-parsed entries — creates JournalEntry + JournalSplits and links CsvImportRow records.

```python
class CsvBookkeeper:
    """Create journal entries from CSV-parsed splits."""

    def __init__(self, session: AsyncSession, mapper: AccountMapper) -> None:
        self._session = session
        self._mapper = mapper

    async def create_entry(
        self,
        entity_id: uuid.UUID,
        wallet: Wallet,
        entry_data: ParsedEntry,
        timestamp: datetime,
    ) -> JournalEntry:
        entry = JournalEntry(
            entity_id=entity_id,
            entry_type=entry_data.entry_type,
            description=f"Binance CSV: {entry_data.entry_type} at {timestamp}",
            timestamp=timestamp,
        )
        self._session.add(entry)
        await self._session.flush()

        for ps in entry_data.splits:
            account = await self._resolve_account(ps, wallet)
            split = JournalSplit(
                journal_entry_id=entry.id,
                account_id=account.id,
                quantity=ps.quantity,
            )
            self._session.add(split)

        await self._session.flush()
        return entry

    async def _resolve_account(self, ps: ParsedSplit, wallet: Wallet) -> Account:
        """Route to AccountMapper based on subtype."""
        if ps.account_subtype == "cex_asset":
            return await self._mapper.cex_asset(wallet, ps.symbol)
        elif ps.account_subtype == "wallet_expense":
            return await self._mapper.cex_expense(wallet, ps.symbol)
        elif ps.account_subtype == "external_transfer":
            ext_addr = ps.account_params.get("ext_address", "external")
            return await self._mapper.external_transfer(wallet, ps.symbol, ext_addr)
        elif ps.account_subtype == "wallet_income":
            tag = ps.account_params.get("tag", "CEX")
            return await self._mapper.income(wallet, ps.symbol, tag)
        else:
            return await self._mapper.cex_asset(wallet, ps.symbol)
```

### Task 5: Add parse trigger API endpoint
**File**: `src/cryptotax/api/imports.py` (modify)

Add endpoint:
```python
@router.post("/{import_id}/parse", response_model=ParseImportResponse)
async def parse_import(import_id: uuid.UUID, db: DbDep) -> ParseImportResponse:
    """Trigger parsing of a CSV import's rows."""
    repo = CsvImportRepo(db)
    csv_import = await repo.get_by_id(import_id)
    if csv_import is None:
        raise HTTPException(status_code=404, detail="Import not found")
    if csv_import.status not in ("uploaded", "completed"):
        raise HTTPException(status_code=409, detail=f"Import status is '{csv_import.status}', expected 'uploaded'")

    # Get or create CEX wallet for this entity+exchange
    wallet = await _get_or_create_cex_wallet(db, csv_import.entity_id, csv_import.exchange)

    # Parse
    await repo.update_status(csv_import.id, "parsing")
    parser = BinanceCsvParser(db, csv_import.entity_id, wallet)
    stats = await parser.parse_import(csv_import)

    # Update import status
    final_status = "completed" if stats.errors == 0 else "completed"
    await repo.update_status(csv_import.id, final_status, parsed_count=stats.parsed, error_count=stats.errors)
    await db.commit()

    return ParseImportResponse(
        import_id=csv_import.id,
        total=stats.total,
        parsed=stats.parsed,
        errors=stats.errors,
        skipped=stats.skipped,
    )
```

Add `ParseImportResponse` schema to `src/cryptotax/api/schemas/imports.py`:
```python
class ParseImportResponse(BaseModel):
    import_id: uuid.UUID
    total: int
    parsed: int
    errors: int
    skipped: int
```

Add helper `_get_or_create_cex_wallet`:
```python
async def _get_or_create_cex_wallet(
    db: AsyncSession, entity_id: uuid.UUID, exchange: str
) -> CEXWallet:
    """Get or create a CEX wallet for this entity+exchange combo."""
    from cryptotax.db.models.wallet import CEXWallet
    result = await db.execute(
        select(CEXWallet).where(
            CEXWallet.entity_id == entity_id,
            CEXWallet.exchange == exchange,
        )
    )
    wallet = result.scalar_one_or_none()
    if wallet is None:
        wallet = CEXWallet(
            entity_id=entity_id,
            exchange=exchange,
            label=f"{exchange.title()} CSV Import",
            wallet_type="cex",
        )
        db.add(wallet)
        await db.flush()
    return wallet
```

### Task 6: Handle mixed-operation timestamp groups
**File**: `src/cryptotax/parser/cex/binance_csv.py`

The CSV has cases where a timestamp contains BOTH a trade AND an internal transfer (e.g., line 3-7 of the real CSV: buy + spend + fee + 2 transfer rows at 08:45:30). The `_parse_group` method needs to split these into sub-groups:

```python
def _parse_group(self, utc_time: str, rows: list[CsvImportRow]) -> list[ParsedEntry]:
    """Split mixed groups and dispatch each sub-group to handler."""
    # Separate by operation category
    trade_rows = [r for r in rows if r.operation in TRADE_OPS]
    convert_rows = [r for r in rows if r.operation == "Binance Convert"]
    deposit_rows = [r for r in rows if r.operation == "Deposit"]
    withdraw_rows = [r for r in rows if r.operation == "Withdraw"]
    p2p_rows = [r for r in rows if r.operation == "P2P Trading"]
    transfer_rows = [r for r in rows if "Transfer Between" in r.operation]
    other_rows = [r for r in rows if r not in (
        trade_rows + convert_rows + deposit_rows + withdraw_rows + p2p_rows + transfer_rows
    )]

    entries = []
    if trade_rows:
        entries.append(self._handle_spot_trade(trade_rows))
    if convert_rows:
        entries.append(self._handle_convert(convert_rows))
    for r in deposit_rows:
        entries.append(self._handle_deposit(r))
    for r in withdraw_rows:
        entries.append(self._handle_withdraw(r))
    for r in p2p_rows:
        entries.append(self._handle_p2p(r))
    if transfer_rows:
        entries.append(self._handle_internal_transfer(transfer_rows))
    for r in other_rows:
        # Unknown ops — mark as skipped (Phase 10 will handle earn/futures/margin)
        r.status = "skipped"
        r.error_message = f"Operation '{r.operation}' not handled in Phase 9"

    return entries

TRADE_OPS = {
    "Transaction Buy", "Transaction Spend", "Transaction Fee",
    "Transaction Sold", "Transaction Revenue",
}
```

### Task 7: Update frontend to trigger parse and show results
**File**: `web/src/pages/Imports.tsx` (modify)

Add "Parse" button to each import in the history table (only when status="uploaded"):
```tsx
<button onClick={() => parseImport(imp.id)} disabled={imp.status !== "uploaded"}>
  Parse
</button>
```

**File**: `web/src/api/imports.ts` (modify)
Add:
```typescript
export interface ParseResult {
  import_id: string
  total: number
  parsed: number
  errors: number
  skipped: number
}

export async function parseImport(importId: string): Promise<ParseResult> {
  return apiFetch(`/imports/${importId}/parse`, { method: 'POST' })
}
```

**File**: `web/src/hooks/useImports.ts` (modify)
Add:
```typescript
export function useParseImport() {
  const queryClient = useQueryClient()
  return useMutation({
    mutationFn: (importId: string) => parseImport(importId),
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['imports'] })
    },
  })
}
```

### Task 8: Write comprehensive tests
**New file**: `tests/unit/test_binance_csv_parser.py`

Test categories:

1. **Row grouping**: Verify rows with same UTC_Time are grouped together
2. **Spot buy**: 3 rows (Buy/Spend/Fee) → SWAP entry with correct splits
3. **Spot sell**: 3 rows (Sold/Revenue/Fee) → SWAP entry
4. **Binance Convert**: 2 rows → SWAP entry
5. **Deposit**: 1 row → DEPOSIT entry (balanced: cex_asset + external_transfer)
6. **Withdraw**: 1 row → WITHDRAWAL entry (balanced)
7. **P2P Trading**: 1 row → DEPOSIT entry
8. **Internal transfer**: 2 mirrored rows → TRANSFER entry (sums to zero)
9. **Mixed timestamp**: Buy + Transfer at same time → 2 separate entries
10. **Unknown operation**: Rows marked as skipped with message
11. **Parse trigger API**: POST /api/imports/{id}/parse returns stats
12. **Error recording**: Invalid row (e.g. non-numeric change) → error status + message
13. **Re-parse protection**: Already-parsed rows are not re-parsed
14. **CEX wallet auto-creation**: First parse creates wallet, second uses existing

Real CSV data patterns used as test fixtures (from binance_export.csv).

### Task 9: TypeScript compilation check + backend tests
Verify:
- `python -m pytest tests/ -x -q` — all tests pass (target: 408 + ~15 new = ~423)
- `ruff check src/` — 0 lint errors
- `cd web && npx tsc --noEmit` — 0 TypeScript errors

## Verification
1. `python -m pytest tests/ -x -q` — all existing + new tests pass
2. `ruff check src/` — 0 lint errors
3. `cd web && npx tsc --noEmit` — 0 TS errors
4. Spot buy CSV rows → balanced SWAP journal entry with fee as expense
5. Spot sell CSV rows → balanced SWAP journal entry
6. Binance Convert rows → balanced SWAP entry
7. Deposit → balanced DEPOSIT entry (cex_asset + external_transfer)
8. Withdraw → balanced WITHDRAWAL entry
9. P2P → balanced DEPOSIT entry
10. Internal transfer rows → TRANSFER entry summing to zero per symbol
11. Unknown operations → rows marked "skipped" with message (not error)
12. POST /api/imports/{id}/parse → returns parse stats (total, parsed, errors, skipped)
13. Parsed rows have journal_entry_id set; error rows have error_message set

## Risks
- **Timestamp grouping ambiguity**: Two unrelated trades could theoretically have the same timestamp. Mitigated by sub-group splitting (Task 6) — trade rows and transfer rows at the same time produce separate entries.
- **Decimal precision**: Binance uses scientific notation (e.g. "-8E-8"). Python `Decimal()` handles this natively.
- **Convert across accounts**: Some converts span Spot+Funding accounts (line 169-172 of real CSV). The handler treats all Convert rows at same timestamp as one swap regardless of account — correct behavior since they represent one logical conversion.
